{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nfrom gensim.models.fasttext import FastText # build and train Fast Text model\nfrom gensim.models import Word2Vec # to Load the saved model\nfrom gensim.models.fasttext import load_facebook_model\nfrom tabulate import tabulate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T16:16:16.377092Z","iopub.execute_input":"2024-04-22T16:16:16.377467Z","iopub.status.idle":"2024-04-22T16:16:36.023600Z","shell.execute_reply.started":"2024-04-22T16:16:16.377437Z","shell.execute_reply":"2024-04-22T16:16:36.022363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading pre-trained FastText model","metadata":{}},{"cell_type":"code","source":"# Download the pre-trained FastText word embeddings for English (300-dimensional vectors) from the Facebook AI repository\n! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n\n# Uncompress the downloaded file using gunzip, so that it can be used by the FastText library\n! gunzip \"cc.en.300.bin.gz\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:16:36.025594Z","iopub.execute_input":"2024-04-22T16:16:36.026205Z","iopub.status.idle":"2024-04-22T16:18:26.345803Z","shell.execute_reply.started":"2024-04-22T16:16:36.026172Z","shell.execute_reply":"2024-04-22T16:18:26.342451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading needed packages for Text Preprocessing","metadata":{}},{"cell_type":"code","source":"# Download the WordNet corpus from NLTK, saving it to the specified directory\nnltk.download('wordnet', \"/kaggle/working/nltk_data/\")\n\n# Download the OMW-1.4 corpus from NLTK, saving it to the specified directory\nnltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n\n# Unzip the WordNet corpus file, extracting it to the corpora directory\n! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n\n# Unzip the OMW-1.4 corpus file, extracting it to the corpora directory\n! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\n\n# Add the custom NLTK data directory to the list of paths that NLTK searches for data\nnltk.data.path.append(\"/kaggle/working/nltk_data/\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:26.350019Z","iopub.execute_input":"2024-04-22T16:18:26.350538Z","iopub.status.idle":"2024-04-22T16:18:31.591221Z","shell.execute_reply.started":"2024-04-22T16:18:26.350494Z","shell.execute_reply":"2024-04-22T16:18:31.590028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading English stopping words","metadata":{}},{"cell_type":"code","source":"# Create a set of English stopwords\nen_stop = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:31.595138Z","iopub.execute_input":"2024-04-22T16:18:31.595541Z","iopub.status.idle":"2024-04-22T16:18:31.614746Z","shell.execute_reply.started":"2024-04-22T16:18:31.595500Z","shell.execute_reply":"2024-04-22T16:18:31.612647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Text data from Yelp dataset","metadata":{}},{"cell_type":"code","source":"# Read in the Yelp dataset from a JSON file, specifying that each line is a separate JSON object\nyelp_datafile = pd.read_json(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_tip.json\", lines=True)\n\n# Print out the list of all column names in the dataset\nprint('List of all columns')\nprint(list(yelp_datafile))\n\n# Subset the data to prepare it for training a gensim fastText model\n# Select only the \"text\" column, which contains the text data we want to analyze\nall_sentences = list(yelp_datafile['text'])\n\n# Select a subset of the text data, taking only the first 1000 samples\npart_of_sentences = all_sentences[0:1000]\n\n# Print out a few examples of the sentences in the subset, to get an idea of what the data looks like\nprint(\"\\nSamples of Sentences\\n {}\".format(part_of_sentences[0:10]))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:31.616794Z","iopub.execute_input":"2024-04-22T16:18:31.617218Z","iopub.status.idle":"2024-04-22T16:18:43.701305Z","shell.execute_reply.started":"2024-04-22T16:18:31.617184Z","shell.execute_reply":"2024-04-22T16:18:43.699876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining a lemmatizer object to be used later","metadata":{}},{"cell_type":"code","source":"# Defining lemmatizer object\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:43.703073Z","iopub.execute_input":"2024-04-22T16:18:43.703570Z","iopub.status.idle":"2024-04-22T16:18:43.710223Z","shell.execute_reply.started":"2024-04-22T16:18:43.703530Z","shell.execute_reply":"2024-04-22T16:18:43.708790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function for Text Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_text(review):\n    review = re.sub(r'\\s+', ' ', review, flags=re.I) # Remove extra white space from text\n\n    review = re.sub(r'\\W', ' ', str(review)) # Remove all the special characters from text\n\n    review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review) # Remove all single characters from text\n    \n    review = re.sub(r'[^a-zA-Z\\s]', '', review) # Remove any character that isn't alphabetical\n\n    review = review.lower() # Converting to Lowercase\n    \n    # Word tokenization \n    tokens = review.split()\n    \n    # Applying lemmatization\n    lemma_txt = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Removing stopping words\n    lemma_no_stop_txt = [word for word in lemma_txt if word not in en_stop]\n    \n    # Drop words less than 3 characters\n    tokens = [word for word in tokens if len(word) > 3]\n    \n    # Getting unique words\n    indices = np.unique(tokens, return_index=True)[1]\n    \n    # Getting the original sorting of the unique words\n    cleaned_unique_review = np.array(tokens)[np.sort(indices)].tolist()\n    \n    return cleaned_unique_review","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:43.712416Z","iopub.execute_input":"2024-04-22T16:18:43.713577Z","iopub.status.idle":"2024-04-22T16:18:43.725346Z","shell.execute_reply.started":"2024-04-22T16:18:43.713530Z","shell.execute_reply":"2024-04-22T16:18:43.724138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_reviews = [ process_text(review) for review in part_of_sentences]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:43.728453Z","iopub.execute_input":"2024-04-22T16:18:43.728919Z","iopub.status.idle":"2024-04-22T16:18:46.674355Z","shell.execute_reply.started":"2024-04-22T16:18:43.728884Z","shell.execute_reply":"2024-04-22T16:18:46.673085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part of training data","metadata":{}},{"cell_type":"code","source":"print(cleaned_reviews[:10])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:46.676007Z","iopub.execute_input":"2024-04-22T16:18:46.676422Z","iopub.status.idle":"2024-04-22T16:18:46.682762Z","shell.execute_reply.started":"2024-04-22T16:18:46.676388Z","shell.execute_reply":"2024-04-22T16:18:46.681379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to train the custom FastText and save it in disk","metadata":{}},{"cell_type":"code","source":"def train_Fasttext(sentences,embedding_size,window_size,min_word,down_sampling,Save_model_filename):\n    fast_Text_model = FastText(sentences,\n    vector_size=embedding_size, # Dimensionality of the word vectors. ,\n    window=window_size,\n    min_count=min_word, # The model ignores all words with total frequency lower than this.\n    sample=down_sampling, # threshold which higher-frequency words are randomly down sampled\n    workers = 4, # Num threads to train the model (faster training with multicore comp.)\n    sg=1, # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n    epochs=100) # Number of iterations (epochs) over the corpus\n\n    fast_Text_model.save(Save_model_filename) # Save fastText gensim model","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:46.686323Z","iopub.execute_input":"2024-04-22T16:18:46.686726Z","iopub.status.idle":"2024-04-22T16:18:46.699340Z","shell.execute_reply.started":"2024-04-22T16:18:46.686683Z","shell.execute_reply":"2024-04-22T16:18:46.698464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nembedding_size = 300  # Dimension of the word embeddings\nwindow_size = 5  # Context window size\nmin_word = 1  # Minimum word count threshold\ndown_sampling = 1e-2  # Downsampling rate for frequent words\n\n# Train the FastText model with the given parameters\ntrain_Fasttext(cleaned_reviews, embedding_size, window_size, min_word, down_sampling, \"Custom_FastText\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:18:46.700984Z","iopub.execute_input":"2024-04-22T16:18:46.701683Z","iopub.status.idle":"2024-04-22T16:19:06.553886Z","shell.execute_reply.started":"2024-04-22T16:18:46.701627Z","shell.execute_reply":"2024-04-22T16:19:06.552942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the saved custom model from disk","metadata":{}},{"cell_type":"code","source":"# Load saved gensim fastText model\nfast_Text_model = Word2Vec.load(\"/kaggle/working/Custom_FastText\") ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:19:06.555180Z","iopub.execute_input":"2024-04-22T16:19:06.556087Z","iopub.status.idle":"2024-04-22T16:19:07.556999Z","shell.execute_reply.started":"2024-04-22T16:19:06.556054Z","shell.execute_reply":"2024-04-22T16:19:07.555865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading pretrained FastText ","metadata":{}},{"cell_type":"code","source":"# Load pretrained fastText word embeddings\npretrained_fastText_en = load_facebook_model('/kaggle/working/cc.en.300.bin')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:19:07.558459Z","iopub.execute_input":"2024-04-22T16:19:07.558825Z","iopub.status.idle":"2024-04-22T16:21:49.878435Z","shell.execute_reply.started":"2024-04-22T16:19:07.558793Z","shell.execute_reply":"2024-04-22T16:21:49.877014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting the top n similar & dissimilar words for a particular word using custom & pre-trained FastText","metadata":{}},{"cell_type":"code","source":"# Get the list of words from the custom-trained model's vocabulary\nwords = list(fast_Text_model.wv.key_to_index)\n\n# Iterate over the list of words, analyzing every 10th word (adjust this value for more or less frequent output)\nfor i in range(len(words)):\n    if i % 10 == 0:\n        print(f\"Analyzing word: {words[i]}\\n\")\n\n        # Get the top 10 similar words to the current word using the custom-trained model\n        similar_words_custom = fast_Text_model.wv.most_similar(words[i], topn=10)\n\n        # Get the top 10 dissimilar (opposite) words to the current word using the custom-trained model\n        opposite_words_custom = fast_Text_model.wv.most_similar(negative=[words[i]], topn=10)\n\n        # Get the top 10 similar words to the current word using the pre-trained English model\n        similar_words_pretrained = pretrained_fastText_en.wv.most_similar(words[i], topn=10)\n\n        # Get the top 10 dissimilar (opposite) words to the current word using the pre-trained English model\n        opposite_words_pretrained = pretrained_fastText_en.wv.most_similar(negative=[words[i]], topn=10)\n\n        # Create tables to display the results using the tabulate library\n        table_custom_similar = tabulate(similar_words_custom, headers=['Similar Word', 'Similarity'], tablefmt='github')\n        table_custom_opposite = tabulate(opposite_words_custom, headers=['Opposite Word', 'Similarity'], tablefmt='github')\n        table_pretrained_similar = tabulate(similar_words_pretrained, headers=['Similar Word', 'Similarity'], tablefmt='github')\n        table_pretrained_opposite = tabulate(opposite_words_pretrained, headers=['Opposite Word', 'Similarity'], tablefmt='github')\n\n        # Print the tables to display the results\n        print(\"Top 10 similar words (custom model):\")\n        print(table_custom_similar)\n        print(\"\\nTop 10 opposite words (custom model):\")\n        print(table_custom_opposite)\n        print(\"\\nTop 10 similar words (pre-trained model):\")\n        print(table_pretrained_similar)\n        print(\"\\nTop 10 opposite words (pre-trained model):\")\n        print(table_pretrained_opposite)\n        print(\"\\n\" + \"-\"*40 + \"\\n\")  # Separator for readability","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:21:49.880235Z","iopub.execute_input":"2024-04-22T16:21:49.880933Z","iopub.status.idle":"2024-04-22T16:23:05.627755Z","shell.execute_reply.started":"2024-04-22T16:21:49.880895Z","shell.execute_reply":"2024-04-22T16:23:05.626128Z"},"trusted":true},"execution_count":null,"outputs":[]}]}